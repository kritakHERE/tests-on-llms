import nltk
import torch
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2Model

# Sample sentence for comparison
sentence = "The quick brown fox jumps over the lazy dog"
tokens = word_tokenize(sentence.lower())

# ---------------- 1. N-GRAM MODEL (Bigram Probabilities) ----------------
from collections import Counter
from nltk.util import ngrams

def compute_ngram_probabilities(tokens, n=2):
    bigrams = list(ngrams(tokens, 2))
    bigram_counts = Counter(bigrams)
    unigram_counts = Counter(tokens)
    probabilities = {bigram: bigram_counts[bigram] / unigram_counts[bigram[0]] for bigram in bigram_counts}
    return probabilities

ngram_probs = compute_ngram_probabilities(tokens)
ngram_next_words = {word: prob for (w1, w2), prob in ngram_probs.items() if w1 == "brown"}

# ---------------- 2. CBOW (Word2Vec) MODEL ----------------
word2vec_model = Word2Vec(sentences=[tokens], vector_size=50, window=2, min_count=1, sg=0)

def get_cbow_probabilities(context_word):
    if context_word in word2vec_model.wv:
        similar_words = word2vec_model.wv.most_similar(context_word, topn=5)
        return {word: prob for word, prob in similar_words}
    return {}

cbow_next_words = get_cbow_probabilities("fox")

# ---------------- 3. BERT MODEL (Self-Attention) ----------------
bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = BertModel.from_pretrained("bert-base-uncased", output_attentions=True)

def get_bert_attention(sentence):
    inputs = bert_tokenizer(sentence, return_tensors="pt")
    outputs = bert_model(**inputs)
    attention = outputs.attentions  # Extract self-attention layers

    # Get attention from the last layer
    last_layer_attention = attention[-1].squeeze(0).mean(dim=0).detach().numpy()
    return last_layer_attention

bert_attention_matrix = get_bert_attention(sentence)

# ---------------- 4. GPT-2 MODEL (Self-Attention) ----------------
gpt_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
gpt_model = GPT2Model.from_pretrained("gpt2", output_attentions=True)

def get_gpt_attention(sentence):
    inputs = gpt_tokenizer(sentence, return_tensors="pt")
    outputs = gpt_model(**inputs)
    attention = outputs.attentions  # Extract self-attention layers

    # Get attention from the last layer
    last_layer_attention = attention[-1].squeeze(0).mean(dim=0).detach().numpy()
    return last_layer_attention

gpt_attention_matrix = get_gpt_attention(sentence)

# ---------------- PLOTTING RESULTS ----------------

# Plot Probability Distribution for N-gram & CBOW
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
models = ["N-gram", "CBOW"]
word_predictions = [ngram_next_words, cbow_next_words]

for i, (model, predictions) in enumerate(zip(models, word_predictions)):
    ax = axes[i]
    words = list(predictions.keys())
    probs = list(predictions.values())

    if not words:
        words = ["No Prediction"]
        probs = [0]

    ax.bar(words, probs, color=['b', 'g', 'r', 'c', 'm'])
    ax.set_title(f"{model} Model")
    ax.set_ylabel("Probability")
    ax.set_xlabel("Next Word")
    ax.set_ylim(0, 1)
    ax.set_xticklabels(words, rotation=30)

plt.suptitle("Probability Distribution for N-gram & CBOW")
plt.tight_layout()
plt.savefig("ngram_cbow_distribution.png")
plt.show()

# Plot Attention Matrices for BERT & GPT-2
fig, axes = plt.subplots(1, 2, figsize=(14, 6))
sns.heatmap(bert_attention_matrix, ax=axes[0], cmap="Blues", xticklabels=tokens, yticklabels=tokens)
axes[0].set_title("BERT Self-Attention Matrix")
sns.heatmap(gpt_attention_matrix, ax=axes[1], cmap="Reds", xticklabels=tokens, yticklabels=tokens)
axes[1].set_title("GPT-2 Self-Attention Matrix")

plt.suptitle("Self-Attention in BERT & GPT-2")
plt.savefig("bert_gpt_attention.png")
plt.show()

print("Graphs saved as 'ngram_cbow_distribution.png' and 'bert_gpt_attention.png'")
