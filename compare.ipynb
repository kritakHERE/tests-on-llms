import nltk
import torch
import torch.nn.functional as F
from nltk.util import ngrams
from collections import Counter
import numpy as np

# Sample text corpus with some overlapping words
corpus_list = [
    "The cat sat on the mat and looked around.",
    "A dog ran across the park chasing a ball.",
    "The cat and the dog played together in the garden.",
    "Running fast, the athlete won the gold medal.",
    "The bird flew over the garden where the cat and dog played."
]

# Function to generate N-grams
def generate_ngrams(text, n):
    tokens = text.lower().split()
    return list(ngrams(tokens, n))

# Processing all texts and collecting n-gram counts
bigram_counts = Counter()
trigram_counts = Counter()
for corpus in corpus_list:
    bigram_counts.update(generate_ngrams(corpus, 2))
    trigram_counts.update(generate_ngrams(corpus, 3))

# Function for Laplace Smoothing
def laplace_smoothing(ngram_counts, vocab_size, alpha=1):
    total_ngrams = sum(ngram_counts.values())
    return {ngram: (count + alpha) / (total_ngrams + alpha * vocab_size) for ngram, count in ngram_counts.items()}

# Applying Laplace smoothing
vocab_size = len(set(" ".join(corpus_list).lower().split()))  # Unique words in corpus
laplace_bigram = laplace_smoothing(bigram_counts, vocab_size)

# -------------------
# Attention Mechanism
# -------------------

class SimpleAttention(torch.nn.Module):
    def __init__(self, embed_dim):
        super(SimpleAttention, self).__init__()
        self.query = torch.nn.Linear(embed_dim, embed_dim)
        self.key = torch.nn.Linear(embed_dim, embed_dim)
        self.value = torch.nn.Linear(embed_dim, embed_dim)

    def forward(self, embeddings):
        q = self.query(embeddings)
        k = self.key(embeddings)
        v = self.value(embeddings)

        attention_scores = torch.matmul(q, k.T) / np.sqrt(k.shape[-1])
        attention_weights = F.softmax(attention_scores, dim=-1)
        return torch.matmul(attention_weights, v)

# Fake word embeddings (just for demo, normally from pre-trained models)
word_embeddings = {word: torch.randn(10) for word in set(" ".join(corpus_list).lower().split())}

# Attention example: Predicting next word based on "the cat"
context_words = ["the", "cat"]
context_vectors = torch.stack([word_embeddings[word] for word in context_words])
attention_model = SimpleAttention(embed_dim=10)
context_representation = attention_model(context_vectors)

# Show results
print("Bigram Frequencies (Raw Counts):", bigram_counts.most_common(5))
print("Bigram Probabilities (Laplace Smoothing):", {k: round(v, 4) for k, v in list(laplace_bigram.items())[:5]})
print("Attention-based Context Representation:", context_representation)

